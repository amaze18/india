# -*- coding: utf-8 -*-
"""Copy of docGPT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M64Fak0I3PIqG4QX2jvO17y5uNp4czzq

## Install required dependencies to run this notebook **bold text**
"""

#Make following parameter 1 if you have restarted using this colab notebook
#after a restart of kernel or re-logging into gmail account

from langchain.llms import OpenAI
from langchain import PromptTemplate, LLMChain

import gradio as gr
import random
import time
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.vectorstores import Chroma
from langchain.llms import GPT4All, LlamaCpp
from langchain.document_loaders import TextLoader, PDFMinerLoader, CSVLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.docstore.document import Document
from langchain import HuggingFacePipeline
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.embeddings import HuggingFaceInstructEmbeddings
from dotenv import load_dotenv
from multiprocessing import Pool
from tqdm import tqdm
from langchain.vectorstores import FAISS
from langchain.document_loaders import (
    CSVLoader,
    EverNoteLoader,
    PDFMinerLoader,
    TextLoader,
    UnstructuredEmailLoader,
    UnstructuredEPubLoader,
    UnstructuredHTMLLoader,
    UnstructuredMarkdownLoader,
    UnstructuredODTLoader,
    UnstructuredPowerPointLoader,
    UnstructuredWordDocumentLoader,
)
import os
import glob
import argparse

from time import sleep
from multiprocessing import Process
chunk_size = 512 #512
chunk_overlap = 50
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.vectorstores import Chroma
from langchain.llms import GPT4All, LlamaCpp

messages = []
kill = 0

"""# **Embeddings model object to vectorize documents **"""

global qa

#hf = HuggingFaceEmbeddings()
from langchain.embeddings.openai import OpenAIEmbeddings
hf= OpenAIEmbeddings(model="text-embedding-ada-002", openai_api_key=api_key)

"""# **All the documents are fetched and stored in vector database here ::**"""
template = """Question: {question}
prompt = PromptTemplate(template=template, input_variables=["question"])


documents = []
a=glob.glob("source_documents/*.txt")
for i in range(len(a)):
        print(a[i])
        documents.extend(TextLoader(a[i]).load())
        print(TextLoader(a[i]).load())

a=glob.glob("source_documents/*.html")

try:
  for i in range(len(a)):
        print(i)

        documents.extend(UnstructuredHTMLLoader(a[i]).load())
except:
  print(i)

a=glob.glob("source_documents/*.pdf")
for i in range(len(a)):

        documents.extend(PDFMinerLoader(a[i]).load())

a=glob.glob("source_documents/*.csv")
for i in range(len(a)):

        documents.extend(CSVLoader(a[i]).load())

a=glob.glob("source_documents/*.ppt")
for i in range(len(a)):

        documents.extend(UnstructuredPowerPointLoader(a[i]).load())

a=glob.glob("source_documents/*.pptx")
for i in range(len(a)):

        documents.extend(UnstructuredPowerPointLoader(a[i]).load())


a=glob.glob("source_documents/*.docx")
for i in range(len(a)):

        documents.extend(UnstructuredWordDocumentLoader(a[i]).load())

a=glob.glob("source_documents/*.ppt")
for i in range(len(a)):

        documents.extend(UnstructuredPowerPointLoader(a[i]).load())

#print(documents)

chunk_size = 1024
chunk_size = 512
text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
texts_isb = text_splitter.split_documents(documents)

db = FAISS.from_documents(texts_isb, hf)


query = "Who is Anupam"
docs = db.similarity_search(query)
docs_and_scores = db.similarity_search_with_score(query)
docs_and_scores[0]
db.save_local("faiss_index_anupam")

"""# **LLM object creation starts here ::**"""

docs_and_scores = db.similarity_search_with_score(query)

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
import openai

chunk_size = 2048
chunk_size = 1024
chunk_size = 512

retriever = db.as_retriever(search_type='similarity', search_kwargs={"k": 5} )
#do not increase k beyond 3, else
callbacks = []
openai.api_key = api_key
llm = OpenAI(model='text-davinci-003',temperature=0, openai_api_key=api_key)
qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True)

memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

#qa = ConversationalRetrievalChain.from_llm(
 #   OpenAI(model='text-davinci-003',temperature=0,openai_api_key=api_key ),
  #  retriever,
   # memory=memory
#)
answer = qa(query)

# Modify the tone using OpenAI's ChatCompletion
response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a helpful and friendly chatbot who converts text to a very friendly tone."},
        {"role": "user", "content": f"{answer}"}
    ]
)

def chat_gpt(qa, question):
   
    query = question
    res = qa(query)
    response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a helpful and friendly chatbot who answers based on contect provided in very friendly tone."},
        {"role": "user", "content": f"{res}"}
    ])
    #print(res)
    answer= response["choices"][0]["message"]["content"]
    docs = db.similarity_search(query)


    doc_list=[]
    for document in docs:
            
            metadata=document.metadata["source"]
            doc_list.append(document)

    return answer #, docs[0],metadata





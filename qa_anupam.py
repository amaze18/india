# -*- coding: utf-8 -*-
"""Copy of docGPT.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1M64Fak0I3PIqG4QX2jvO17y5uNp4czzq

## Install required dependencies to run this notebook **bold text**
"""

#Make following parameter 1 if you have restarted using this colab notebook
#after a restart of kernel or re-logging into gmail account

from langchain.llms import OpenAI
from langchain import PromptTemplate, LLMChain
import gradio as gr
import random
import time
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.vectorstores import Chroma
from langchain.llms import GPT4All, LlamaCpp
from langchain.document_loaders import TextLoader, PDFMinerLoader, CSVLoader
from langchain.text_splitter import CharacterTextSplitter
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain.docstore.document import Document
from langchain import HuggingFacePipeline
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.embeddings import HuggingFaceInstructEmbeddings
from dotenv import load_dotenv
from multiprocessing import Pool
from tqdm import tqdm
from langchain.vectorstores import FAISS
from langchain.document_loaders import (
    CSVLoader,
    EverNoteLoader,
    PDFMinerLoader,
    TextLoader,
    UnstructuredEmailLoader,
    UnstructuredEPubLoader,
    UnstructuredHTMLLoader,
    UnstructuredMarkdownLoader,
    UnstructuredODTLoader,
    UnstructuredPowerPointLoader,
    UnstructuredWordDocumentLoader,
)
import os
import glob
import argparse

from time import sleep
from multiprocessing import Process
chunk_size = 512 #512
chunk_overlap = 50
from dotenv import load_dotenv
from langchain.chains import RetrievalQA
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler
from langchain.vectorstores import Chroma
from langchain.llms import GPT4All, LlamaCpp
"""# **Embeddings model object to vectorize documents **"""

global qa
import os


SECRET_TOKEN = os.environ["SECRET_TOKEN"] 
openai.api_key = SECRET_TOKEN
messages = []
kill = 0

from langchain.embeddings.openai import OpenAIEmbeddings
hf= OpenAIEmbeddings(model="text-embedding-ada-002", openai_api_key=api_key)

"""# **All the documents are fetched and stored in vector database here ::**"""
template = """Question: {question}
prompt = PromptTemplate(template=template, input_variables=["question"])


documents = []
a=glob.glob("source_documents/*.txt")
for i in range(len(a)):
        print(a[i])
        documents.extend(TextLoader(a[i]).load())
        print(TextLoader(a[i]).load())

a=glob.glob("source_documents/*.html")

try:
  for i in range(len(a)):
        print(i)

        documents.extend(UnstructuredHTMLLoader(a[i]).load())
except:
  print(i)

a=glob.glob("source_documents/*.pdf")
for i in range(len(a)):

        documents.extend(PDFMinerLoader(a[i]).load())

a=glob.glob("source_documents/*.csv")
for i in range(len(a)):

        documents.extend(CSVLoader(a[i]).load())

a=glob.glob("source_documents/*.ppt")
for i in range(len(a)):

        documents.extend(UnstructuredPowerPointLoader(a[i]).load())

a=glob.glob("source_documents/*.pptx")
for i in range(len(a)):

        documents.extend(UnstructuredPowerPointLoader(a[i]).load())


a=glob.glob("source_documents/*.docx")
for i in range(len(a)):

        documents.extend(UnstructuredWordDocumentLoader(a[i]).load())

a=glob.glob("source_documents/*.ppt")
for i in range(len(a)):

        documents.extend(UnstructuredPowerPointLoader(a[i]).load())


chunk_size = 1024
chunk_size = 512
text_splitter = CharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)
texts_isb = text_splitter.split_documents(documents)

db = FAISS.from_documents(texts_isb, hf)
db.save_local("faiss_index_anupam")

from langchain.chains import ConversationalRetrievalChain
from langchain.memory import ConversationBufferMemory
import openai

chunk_size = 2048
chunk_size = 1024
chunk_size = 512

def chat_gpt(qa, question):

    retriever = db.as_retriever(search_type='similarity', search_kwargs={"k": 5} )#do not increase k beyond 3, else
    openai.api_key = api_key
    llm = OpenAI(model='text-davinci-003',temperature=0, openai_api_key=api_key)
    qa = RetrievalQA.from_chain_type(llm=llm, chain_type="stuff", retriever=retriever, return_source_documents=True)
    
    memory = ConversationBufferMemory(memory_key="chat_history", return_messages=True)

    answer = qa(query)
    
    # Modify the tone using OpenAI's ChatCompletion
    response = openai.ChatCompletion.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": "You are a helpful and friendly chatbot who converts text to a very friendly tone."},
            {"role": "user", "content": f"{answer}"}
        ]
    )

    query = question
    res = qa(query)
    response = openai.ChatCompletion.create(
    model="gpt-3.5-turbo",
    messages=[
        {"role": "system", "content": "You are a helpful and friendly chatbot who answers based on contect provided in very friendly tone."},
        {"role": "user", "content": f"{res}"}
    ])
    answer= response["choices"][0]["message"]["content"]
    return answer




